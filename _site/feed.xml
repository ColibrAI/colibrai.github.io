<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-02-18T08:42:18+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">ColibrAI</title><subtitle></subtitle><entry><title type="html">Third Workshop: Participatory AI</title><link href="http://localhost:4000/blog/2026/01/27/third-workshop-notes.html" rel="alternate" type="text/html" title="Third Workshop: Participatory AI" /><published>2026-01-27T00:00:00+01:00</published><updated>2026-01-27T00:00:00+01:00</updated><id>http://localhost:4000/blog/2026/01/27/third-workshop-notes</id><content type="html" xml:base="http://localhost:4000/blog/2026/01/27/third-workshop-notes.html">&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img src=&quot;/images/blog4.png&quot; style=&quot;width:60%;&quot; /&gt;
  &lt;div style=&quot;margin-top:-8px; font-size:0.9em; color:#555;&quot;&gt;
    Clarote &amp;amp; AI4Media / Better Images of AI / Power/Profit / CC-BY 4.0
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;
The third ColibrAI meeting, led by Aalto, focused on how design and participation can actively reshape AI&apos;s trajectories, examining how institutional, cultural, and infrastructural conditions surrounding AI might be transformed. The guiding concern was not only what AI systems obscure or reproduce, but how their development and governance could be reconfigured through participatory practices.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
A foundational premise framed the discussion: design is not simply the production of discrete objects, but the cultural production of new forms of practice. From this perspective, AI systems are not isolated technical artifacts. They are embedded in social routines, material infrastructures, and epistemic hierarchies. Designing AI, therefore, cannot be separated from redesigning the environments that sustain it. Questions of technical optimization inevitably lead to questions of power, expertise, and whose knowledge is recognized as authoritative.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
This perspective made it necessary to examine participation itself. Participation is frequently presented as inherently democratic, yet it is not automatically emancipatory. It holds the potential for empowerment, but also for illusion. When participatory exercises are added onto systems whose architectures, business models, and predictive logics remain unchanged, inclusion risks becoming symbolic. Participation was therefore framed not as an invitation, but as a negotiation: who defines the problem, who sets the criteria of fairness, and who has the authority to intervene at the infrastructural level.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The traditions of participatory design, particularly those influenced by participatory action research, offered a way to move beyond token engagement. These traditions emphasize mutual learning, distributed experimentation, and infrastructuring rather than isolated product development. This shift from focusing on individual products to examining the larger systems and infrastructures that support them became a key point in the discussion. If AI systems are sustained by data pipelines, governance structures, evaluation metrics, and educational frameworks, then meaningful participation must engage these broader assemblages rather than only the visible interfaces of AI tools.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Within this framework, an important distinction emerged between designing with AI and designing AI. Designing with AI treats it as a creative medium within collaborative or speculative processes. Designing AI positions it as an object of critical intervention, requiring scrutiny of its architectures, training data, and embedded assumptions. This distinction highlights a dual responsibility: experimenting with AI as a medium while simultaneously interrogating the epistemic and infrastructural conditions that shape its operation.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
A central thread throughout the session was whether the dominant terms of AI discourse can be shifted. Instead of focusing on efficiency, automation, and optimization, participatory and speculative design approaches open space for imaginaries grounded in care, plurality, relationality, and collective world-making. Projects such as AI-assisted participatory interventions with queer nightlife communities illustrated how speculative laboratories can cultivate counter-practices and counter-spaces. These initiatives demonstrate that AI can be engaged not only as a tool of standardization but as a medium for questioning and reimagining social and urban futures.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
A review of current research landscapes underscored the importance of this orientation. While design-related AI research is expanding, explicitly justice-centered, decolonial, and participatory approaches remain comparatively marginal. This gap underscores the need for interdisciplinary collaboration that integrates technical expertise, critical theory, and creative practice. Such collaboration does more than merge perspectives; it creates the possibility of a shared epistemic space capable of challenging technocentric and Western-centric assumptions.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
In this framing, community-centric AI is not limited to identifying harms or inviting feedback. It entails building infrastructures that redistribute voice, authority, and responsibility. It requires questioning who is authorized to define problems, imagine futures, and shape technological systems.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
In addition to conceptual discussions on participatory design, the session also included what was described as “Adventure(s) with SCOPUS and AI”. This exercise revisited the literature review process itself as a site of inquiry. By experimenting with different search strings in SCOPUS, combining terms such as AI, participatory design, inclusivity, decolonization, and speculative design, the exploration revealed not only thematic trends but also structural gaps.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Searches that explicitly included inclusivity, intersectionality, or decolonial keywords produced surprisingly few relevant results, especially from design-oriented perspectives. When these terms were removed, the number of results increased substantially, but the justice-centered focus weakened. This “adventure” highlighted how database logics, keyword conventions, and disciplinary boundaries shape what becomes visible as legitimate research. It reinforced a central insight of the session: knowledge production around AI is itself infrastructural and political. Even the act of searching the literature reflects and reproduces dominant imaginaries, revealing how much work remains to articulate and consolidate justice-oriented, participatory approaches within AI and design research.
&lt;/p&gt;</content><author><name>ColibrAI team</name></author><category term="blog" /><summary type="html">The third ColibrAI meeting, led by Aalto, focused on how design and participation can actively reshape AI&apos;s trajectories, examining how institutional, cultural, and infrastructural conditions surrounding AI might be transformed. The guiding concern was not only what AI systems obscure or reproduce, but how their development and governance could be reconfigured through participatory practices.</summary></entry><entry><title type="html">Second Workshop: Decolonize AI</title><link href="http://localhost:4000/blog/2026/01/20/second-workshop-notes.html" rel="alternate" type="text/html" title="Second Workshop: Decolonize AI" /><published>2026-01-20T00:00:00+01:00</published><updated>2026-01-20T00:00:00+01:00</updated><id>http://localhost:4000/blog/2026/01/20/second-workshop-notes</id><content type="html" xml:base="http://localhost:4000/blog/2026/01/20/second-workshop-notes.html">&lt;div style=&quot;text-align:center;&quot;&gt;
  &lt;img src=&quot;/images/blog3.jpg&quot; style=&quot;width:60%;&quot; /&gt;
  &lt;div style=&quot;margin-top:-8px; font-size:0.9em; color:#555;&quot;&gt;
    Michael Kwet, &quot;Digital Colonialism: The Evolution of US Empire&quot;, Transnational Institute Longread
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The second ColibrAI meeting, led by Damstad, approached AI through a decolonial lens, questioning the Western-centric assumptions that shape both AI development and educational practice and examining the epistemological foundations of education, technology, and the idea of progress itself.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The conversation began with a fundamental question: what is education for? Educational research was described as having two core tasks: (1) the theoretical and empirical study of pedagogical practice, and (2) the development of proposals to shape or improve educational contexts. Yet education was not reduced to schooling or vocational training. The concept of Bildung foregrounds education as cultural formation and lifelong self-development. From this perspective, education becomes inseparable from broader societal transformations. This raised a crucial issue: in a world increasingly structured by digital technologies and AI, how are educational practices, institutions, and even notions of personhood being reshaped?
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
From this point, the discussion naturally expanded to digitalization and digitality. Instead of treating digitalization as a sequence of technological innovations, digitality was framed as a condition in which digital technologies are already deeply woven into everyday life. In this postdigital context, the focus shifts away from novelty and toward structural transformation. Digital media shape communication, social organization, perception, and self-understanding. AI, therefore, cannot be treated as a discrete technological breakthrough; it must be understood as part of a broader media ecology that reorganizes cultural and social life.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
This broader view made it necessary to confront the material dimensions of digital systems. The apparent immateriality of “the cloud” and seamless interfaces conceals extensive global infrastructures. From extractive mining and industrial production to data centers and satellite networks, digital technologies are grounded in material processes that are unevenly distributed across the globe. The digital is neither neutral nor weightless. It is entangled with environmental costs, labor exploitation, and geopolitical inequalities. Recognizing this materiality disrupts narratives of frictionless innovation and exposes the structural conditions underpinning AI systems.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
These material considerations led directly to a discussion of sustainability and the future. Drawing on established definitions of sustainable development, the session emphasized justice across generations as well as within the present. Sustainability was framed not as a question of technical efficiency alone, but as a normative and political issue: how should present needs be balanced with those of future generations? Competing logics such as efficiency, sufficiency, and consistency were examined in relation to digital technologies. In this light, AI becomes a force that not only predicts the future but actively shapes collective imaginaries of what the future can or should be.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The focus on sustainability also required attention to the full lifecycle of digital technologies, from development and production to use, disposal, and recycling. Questions emerged about exploitative labor conditions, unequal access to digital infrastructure, disparities in digital skills, linguistic dominance online, and the ongoing export of electronic waste to the Global South. The so-called digital divide appeared not simply as a lack of connectivity, but as a structural configuration of power, knowledge, and resources. AI development, in this context, is inseparable from colonial histories and contemporary global asymmetries.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
This structural analysis was deepened through an examination of the historical entanglement between media technologies and colonial narratives. European science and technological advancement were historically framed as enlightenment and universal progress, while writing, cartography, surveying, and documentation operated as instruments of governance and control. Literacy was elevated as a marker of intelligence, and certain languages and representational systems were privileged over others. These historical dynamics illuminate how contemporary AI systems may continue to privilege specific datasets, epistemologies, and linguistic norms while marginalizing others, often under the appearance of neutrality.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Alternative cartographic practices provided a compelling illustration of this point. Mapping is not a neutral act of depiction; it is an act of world-making. Maps construct hierarchies, perspectives, and boundaries. In a similar way, AI systems participate in world-making through data selection, classification, and prediction. They do not merely mirror reality; they help produce it. This insight underscores the importance of interrogating the epistemological assumptions embedded in data collection, modeling, and evaluation processes.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Against this backdrop, participation itself came under scrutiny. While digital infrastructures are frequently associated with democratization and openness, they often reproduce asymmetries of knowledge, resources, and influence. The idea of “Potemkin AI”, systems that appear fully automated while relying on hidden human labor, exemplifies how technological narratives can obscure underlying structures. Participation risks becoming performative when it leaves foundational power relations untouched. In response, the notion of “coliberation” was proposed as a more demanding alternative: not participation as spectacle, but shared critical responsibility and transformative engagement.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The session concluded by returning to the question of the future. As algorithmic prediction becomes increasingly operational, it reshapes decision-making in the present. Prediction shifts from statistical estimation to preemptive action. In educational contexts, this development carries significant consequences. Predictive analytics and automated assessment systems may channel trajectories in advance, narrowing rather than expanding the space of possibility that education is meant to cultivate.
&lt;/p&gt;</content><author><name>ColibrAI team</name></author><category term="blog" /><summary type="html">The second ColibrAI meeting, led by Damstad, approached AI through a decolonial lens, questioning the Western-centric assumptions that shape both AI development and educational practice and examining the epistemological foundations of education, technology, and the idea of progress itself.</summary></entry><entry><title type="html">First Workshop: Ecofeminist AI</title><link href="http://localhost:4000/blog/2025/11/24/first-workshop-notes.html" rel="alternate" type="text/html" title="First Workshop: Ecofeminist AI" /><published>2025-11-24T00:00:00+01:00</published><updated>2025-11-24T00:00:00+01:00</updated><id>http://localhost:4000/blog/2025/11/24/first-workshop-notes</id><content type="html" xml:base="http://localhost:4000/blog/2025/11/24/first-workshop-notes.html">&lt;div style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;/images/blog2.jpg&quot; style=&quot;width:60%;&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The first ColibrAI meeting, led by KTH, brought the group together to establish a shared foundation for thinking about AI as a matter of justice rather than merely technology. The starting point was ecofeminism, which offered that AI is not simply a technical system, but a structure of power. It shapes whose voices are heard, whose needs are prioritized, and whose lives, labor, and environments are made visible or rendered invisible. Although AI often presents itself as neutral and objective, the discussion emphasized how it encodes social, political, and economic values, frequently reproducing hierarchies that long predate digital technologies.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
From this framing of AI as power, attention turned to what remains hidden within AI systems. The concept of epistemic omission, drawing on Gayatri Spivak&apos;s notion of epistemic violence, helped articulate how certain forms of knowledge and certain communities fall outside the frame of mainstream AI design. What is never collected, never labeled, or never modeled has material consequences. Examples ranging from healthcare research and urban planning to smartphone design and crash-test standards illustrated how data gaps shape whose realities matter and whose risks are ignored. The idea of “privilege hazard” further clarified how structural ignorance among designers can become normalized, embedding exclusion into technological systems without appearing intentional.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
This focus on omission naturally extended to labor. AI systems depend on multiple layers of invisible work: the extraction of minerals such as cobalt and lithium, manufacturing under hazardous conditions, the maintenance of energy-intensive data centers, and the global workforce of annotators, moderators, and microworkers who sustain machine learning systems. Much of this labor is low-paid, repetitive, and psychologically taxing, yet remains obscured behind seamless interfaces. The discussion also acknowledged less visible forms of labor, including the work users perform through everyday interactions with digital platforms. Even educational institutions are affected, as technology companies increasingly displace or reshape roles traditionally held by teachers, shifting authority and responsibility within learning environments.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Environmental harm formed another crucial dimension of what lies &quot;under the hood&quot; of AI. Carbon emissions, water consumption, and the rapid growth of electronic waste were not treated as unfortunate side effects, but as structural features of contemporary AI infrastructures. The notion of green colonialism captured how environmental burdens are often displaced onto marginalized regions. Framing these issues together (epistemic omission, invisible labor, and ecological impact) made clear that AI operates as a global supply chain, entangled with extractive economies and uneven power relations.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Against this backdrop, ecofeminist AI was introduced not only as critique but as praxis. Rooted in anti-nuclear, Indigenous, and feminist movements of the 1970s, ecofeminism links environmental and social justice and insists on relationality, care, and plurality. Applied to AI, this perspective calls for reimagining technological development as ecological and justice-centered rather than extractive and profit-driven. It invites questions such as: Who builds? Who benefits? Who is erased? The discussion emphasized the need to move beyond individualized ethics toward structural justice, shifting the focus from compliance and bias metrics to transformations in governance, ownership, and accountability.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
This shift also required epistemic work. Participants reflected on the importance of unlearning dominant narratives of progress and neutrality, critically engaging with existing technologies, and distinguishing between use cases that reinforce injustice and those that may enable resistance. Ideas such as subversion, hacking, optimism, and even &quot;irritating&quot; established systems emerged as strategies for intervening in power structures rather than accepting them as fixed. The idea of community-accountable AI emerged as a challenging and forward-looking question: what would it mean for AI systems to be grounded in genuine collective consent and community agency?
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The session concluded by connecting these principles to concrete action. Justice-oriented initiatives such as DAIR, Data Workers&apos; Inquiry, Data Against Feminicide, and Indigenous AI projects were discussed as examples of alternative approaches. Building on these inspirations, the group outlined plans for an LLM-based storytelling platform designed to surface erased narratives around invisible labor, environmental harm, and data gaps in marginalized domains. Rather than treating storytelling as an add-on, the project positions narrative as a way of making structural injustices legible and contestable.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The first workshop laid the intellectual and political groundwork for ColibrAI. By exposing the hidden infrastructures of AI and articulating ecofeminist AI as a relational, justice-centered alternative, the meeting reframed AI as a contested terrain.
&lt;/p&gt;</content><author><name>ColibrAI team</name></author><category term="blog" /><summary type="html">The first ColibrAI meeting, led by KTH, brought the group together to establish a shared foundation for thinking about AI as a matter of justice rather than merely technology. The starting point was ecofeminism, which offered that AI is not simply a technical system, but a structure of power. It shapes whose voices are heard, whose needs are prioritized, and whose lives, labor, and environments are made visible or rendered invisible. Although AI often presents itself as neutral and objective, the discussion emphasized how it encodes social, political, and economic values, frequently reproducing hierarchies that long predate digital technologies. From this framing of AI as power, attention turned to what remains hidden within AI systems. The concept of epistemic omission, drawing on Gayatri Spivak&apos;s notion of epistemic violence, helped articulate how certain forms of knowledge and certain communities fall outside the frame of mainstream AI design. What is never collected, never labeled, or never modeled has material consequences. Examples ranging from healthcare research and urban planning to smartphone design and crash-test standards illustrated how data gaps shape whose realities matter and whose risks are ignored. The idea of “privilege hazard” further clarified how structural ignorance among designers can become normalized, embedding exclusion into technological systems without appearing intentional. This focus on omission naturally extended to labor. AI systems depend on multiple layers of invisible work: the extraction of minerals such as cobalt and lithium, manufacturing under hazardous conditions, the maintenance of energy-intensive data centers, and the global workforce of annotators, moderators, and microworkers who sustain machine learning systems. Much of this labor is low-paid, repetitive, and psychologically taxing, yet remains obscured behind seamless interfaces. The discussion also acknowledged less visible forms of labor, including the work users perform through everyday interactions with digital platforms. Even educational institutions are affected, as technology companies increasingly displace or reshape roles traditionally held by teachers, shifting authority and responsibility within learning environments. Environmental harm formed another crucial dimension of what lies &quot;under the hood&quot; of AI. Carbon emissions, water consumption, and the rapid growth of electronic waste were not treated as unfortunate side effects, but as structural features of contemporary AI infrastructures. The notion of green colonialism captured how environmental burdens are often displaced onto marginalized regions. Framing these issues together (epistemic omission, invisible labor, and ecological impact) made clear that AI operates as a global supply chain, entangled with extractive economies and uneven power relations. Against this backdrop, ecofeminist AI was introduced not only as critique but as praxis. Rooted in anti-nuclear, Indigenous, and feminist movements of the 1970s, ecofeminism links environmental and social justice and insists on relationality, care, and plurality. Applied to AI, this perspective calls for reimagining technological development as ecological and justice-centered rather than extractive and profit-driven. It invites questions such as: Who builds? Who benefits? Who is erased? The discussion emphasized the need to move beyond individualized ethics toward structural justice, shifting the focus from compliance and bias metrics to transformations in governance, ownership, and accountability. This shift also required epistemic work. Participants reflected on the importance of unlearning dominant narratives of progress and neutrality, critically engaging with existing technologies, and distinguishing between use cases that reinforce injustice and those that may enable resistance. Ideas such as subversion, hacking, optimism, and even &quot;irritating&quot; established systems emerged as strategies for intervening in power structures rather than accepting them as fixed. The idea of community-accountable AI emerged as a challenging and forward-looking question: what would it mean for AI systems to be grounded in genuine collective consent and community agency? The session concluded by connecting these principles to concrete action. Justice-oriented initiatives such as DAIR, Data Workers&apos; Inquiry, Data Against Feminicide, and Indigenous AI projects were discussed as examples of alternative approaches. Building on these inspirations, the group outlined plans for an LLM-based storytelling platform designed to surface erased narratives around invisible labor, environmental harm, and data gaps in marginalized domains. Rather than treating storytelling as an add-on, the project positions narrative as a way of making structural injustices legible and contestable. The first workshop laid the intellectual and political groundwork for ColibrAI. By exposing the hidden infrastructures of AI and articulating ecofeminist AI as a relational, justice-centered alternative, the meeting reframed AI as a contested terrain.</summary></entry><entry><title type="html">ColibrAI is ready to take off!</title><link href="http://localhost:4000/blog/2025/11/18/welcome.html" rel="alternate" type="text/html" title="ColibrAI is ready to take off!" /><published>2025-11-18T00:00:00+01:00</published><updated>2025-11-18T00:00:00+01:00</updated><id>http://localhost:4000/blog/2025/11/18/welcome</id><content type="html" xml:base="http://localhost:4000/blog/2025/11/18/welcome.html">&lt;div style=&quot;text-align:center;&quot;&gt;&lt;img src=&quot;/images/blog1.png&quot; /&gt;&lt;/div&gt;

&lt;p align=&quot;justify&quot;&gt;
The ColibrAI project is now getting started, and the first kick-off meeting has already taken place. The involved teams are led by &lt;a href=&quot;https://payberah.github.io/&quot;&gt;Amir H. Payberah&lt;/a&gt; from KTH Royal Institute of Technology (Sweden), &lt;a href=&quot;https://www.aalto.fi/en/people/andrea-botero-cabrera&quot;&gt;Andrea Botero&lt;/a&gt; from Aalto University (Finland), and &lt;a href=&quot;https://www.abpaed.tu-darmstadt.de/pid/arbeitsbereich_pid/team_pid/details_75904.de.jsp&quot;&gt;Nina Grünberger&lt;/a&gt; from TU Darmstadt (Germany). The ColibrAI project is part of Unite! University&apos;s Seed Funds programme, which brings together three teams from different international universities to combine their expertise in AI, justice, participatory design, and decolonial perspectives.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
The first meeting focused on all three teams getting to know each other. In addition to the team leaders, the ColibrAI project team consists of members from varying status groups of scientific researchers. The main focus was on learning about the various disciplines and their respective approaches to AI, as well as identifying potential interdisciplinary points of contact. The teams from the project universities presented their disciplines, providing a brief introduction to their current projects and research topics, and outlined their various approaches to AI. The project plan and its various phases were then presented to address questions and establish initial agreements on the collaborative work approach in ColibrAI.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
Those of us involved from the participating universities are starting the project with a positive feeling and look forward to the exciting developments in the upcoming months.
&lt;/p&gt;</content><author><name>ColibrAI team</name></author><category term="blog" /><summary type="html">The ColibrAI project is now getting started, and the first kick-off meeting has already taken place. The involved teams are led by Amir H. Payberah from KTH Royal Institute of Technology (Sweden), Andrea Botero from Aalto University (Finland), and Nina Grünberger from TU Darmstadt (Germany). The ColibrAI project is part of Unite! University&apos;s Seed Funds programme, which brings together three teams from different international universities to combine their expertise in AI, justice, participatory design, and decolonial perspectives. The first meeting focused on all three teams getting to know each other. In addition to the team leaders, the ColibrAI project team consists of members from varying status groups of scientific researchers. The main focus was on learning about the various disciplines and their respective approaches to AI, as well as identifying potential interdisciplinary points of contact. The teams from the project universities presented their disciplines, providing a brief introduction to their current projects and research topics, and outlined their various approaches to AI. The project plan and its various phases were then presented to address questions and establish initial agreements on the collaborative work approach in ColibrAI. Those of us involved from the participating universities are starting the project with a positive feeling and look forward to the exciting developments in the upcoming months.</summary></entry></feed>